{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation with flip at 3 different positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# imatges originals:\n",
    "input_dir = \"../../train_10_artists\" \n",
    "\n",
    "# where are we saving the images created:\n",
    "output_dir = \"../../train_10_artists_flip\"\n",
    "\n",
    "# create the folder:\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# save all the original images:\n",
    "image_files = os.listdir(input_dir)\n",
    "\n",
    "for img_file in image_files:\n",
    "    img_path = os.path.join(input_dir, img_file)\n",
    "\n",
    "    # this reads the original image\n",
    "    img = cv2.imread(img_path)\n",
    "    # as we had some errors, we need to find out if it loaded correcty\n",
    "    if img is None:\n",
    "        print(\"No carrega correctament\", img_path)\n",
    "        continue\n",
    "\n",
    "    # horizontal\n",
    "    flipped_img_h = cv2.flip(img, 1)\n",
    "    cv2.imwrite(os.path.join(output_dir, f\"{img_file.split('.')[0]}_flipped_h.jpg\"), flipped_img_h)\n",
    "\n",
    "    # vertical\n",
    "    flipped_img_v = cv2.flip(img, 0)\n",
    "    cv2.imwrite(os.path.join(output_dir, f\"{img_file.split('.')[0]}_flipped_v.jpg\"), flipped_img_v)\n",
    "\n",
    "    # horizontal and vertical\n",
    "    flipped_img_hv = cv2.flip(img, -1)\n",
    "    cv2.imwrite(os.path.join(output_dir, f\"{img_file.split('.')[0]}_flipped_hv.jpg\"), flipped_img_hv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from random import seed # for setting seed\n",
    "import tensorflow\n",
    "from IPython import sys_info\n",
    "\n",
    "import gc # garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'commit_hash': '928881c53',\n",
      " 'commit_source': 'installation',\n",
      " 'default_encoding': 'utf-8',\n",
      " 'ipython_path': 'C:\\\\Users\\\\aluce\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python39\\\\site-packages\\\\IPython',\n",
      " 'ipython_version': '8.18.0',\n",
      " 'os_name': 'nt',\n",
      " 'platform': 'Windows-10-10.0.22621-SP0',\n",
      " 'sys_executable': 'C:\\\\Users\\\\aluce\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\\\python.exe',\n",
      " 'sys_platform': 'win32',\n",
      " 'sys_version': '3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC '\n",
      "                'v.1929 64 bit (AMD64)]'}\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "MY_SEED = 42 # 480 could work too\n",
    "seed(MY_SEED)\n",
    "np.random.seed(MY_SEED)\n",
    "tensorflow.random.set_seed(MY_SEED)\n",
    "\n",
    "print(sys_info())\n",
    "# get module information\n",
    "%pip freeze > frozen-requirements.txt\n",
    "# append system information to file\n",
    "with open(\"frozen-requirements.txt\", \"a\") as file:\n",
    "    file.write(sys_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: \"{'commit_hash': '928881c53',\" (from line 150 of frozen-requirements.txt)\n"
     ]
    }
   ],
   "source": [
    "pip install -r \"frozen-requirements.txt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 126] The specified module could not be found. Error loading \"C:\\Users\\aluce\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available()) \u001b[38;5;66;03m# Ha de ser True\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\__init__.py:141\u001b[0m\n\u001b[0;32m    139\u001b[0m                 err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(ctypes\u001b[38;5;241m.\u001b[39mget_last_error())\n\u001b[0;32m    140\u001b[0m                 err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 141\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    143\u001b[0m     kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_preload_cuda_deps\u001b[39m(lib_folder, lib_name):\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] The specified module could not be found. Error loading \"C:\\Users\\aluce\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available()) # Ha de ser True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 11605646629191735636\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "# print out the CPUs and GPUs\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/25705773/image-cropping-tool-python\n",
    "# because painting images are hella big\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globals\n",
    "\n",
    "DATA_DIR = \"../../\" \n",
    "TRAIN_1_DIR =  \"../../train_10_artists\" \n",
    "\n",
    "TRAIN_DIRS = [TRAIN_1_DIR]\n",
    "TEST_DIR = \"../../test_10_artists\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute in local and virtual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating images in directory: ../../\n",
      "Validating images in directory: ../../test_10_artists\n",
      "Validating images in directory: ../../train_10_artists\n",
      "Valid image: ../../train_10_artists\\143.jpg\n",
      "Valid image: ../../train_10_artists\\209.jpg\n",
      "Valid image: ../../train_10_artists\\217.jpg\n",
      "Valid image: ../../train_10_artists\\241.jpg\n",
      "Valid image: ../../train_10_artists\\270.jpg\n",
      "Valid image: ../../train_10_artists\\298.jpg\n",
      "Valid image: ../../train_10_artists\\302.jpg\n",
      "Valid image: ../../train_10_artists\\313.jpg\n",
      "Valid image: ../../train_10_artists\\315.jpg\n",
      "Valid image: ../../train_10_artists\\355.jpg\n",
      "Valid image: ../../train_10_artists\\384.jpg\n",
      "Valid image: ../../train_10_artists\\391.jpg\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Function to check if a file is an image\n",
    "def is_image(file_path):\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp']\n",
    "    return any(file_path.lower().endswith(ext) for ext in image_extensions)\n",
    "\n",
    "# Function to check if an image file is valid\n",
    "def is_valid_image(file_path):\n",
    "    try:\n",
    "        img = Image.open(file_path)\n",
    "        img.verify()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Invalid image: {file_path} - {e}\")\n",
    "        # Remove the file if it's invalid\n",
    "        os.remove(file_path)\n",
    "        print(f\"Removed file: {file_path}\")\n",
    "        return False\n",
    "\n",
    "# Validate images in a directory\n",
    "def validate_images(directory):\n",
    "    print(f\"Validating images in directory: {directory}\")\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        if os.path.isfile(file_path) and is_image(file_path):\n",
    "            if is_valid_image(file_path):\n",
    "                print(f\"Valid image: {file_path}\")\n",
    "\n",
    "# Validate training and testing directories\n",
    "def validate_directories(*directories):\n",
    "    for directory in directories:\n",
    "        if not os.path.exists(directory):\n",
    "            raise ValueError(f\"Directory does not exist: {directory}\")\n",
    "        if not os.path.isdir(directory):\n",
    "            raise ValueError(f\"Not a directory: {directory}\")\n",
    "\n",
    "# Validate training and testing directories\n",
    "validate_directories(DATA_DIR, TEST_DIR)\n",
    "for train_dir in TRAIN_DIRS:\n",
    "    validate_directories(train_dir)\n",
    "\n",
    "# Validate images in training and testing directories\n",
    "validate_images(DATA_DIR)\n",
    "validate_images(TEST_DIR)\n",
    "for train_dir in TRAIN_DIRS:\n",
    "    validate_images(train_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in the folder TRAIN_1_DIR: 12\n",
      "Number of files in the folder TEST_DIR: 0\n"
     ]
    }
   ],
   "source": [
    "# Get the list of files in the folder\n",
    "files_in_folder = os.listdir(TRAIN_1_DIR)\n",
    "\n",
    "# Count the number of files in the folder\n",
    "num_files_in_folder = len(files_in_folder)\n",
    "\n",
    "print(\"Number of files in the folder TRAIN_1_DIR:\", num_files_in_folder)\n",
    "\n",
    "\n",
    "files_in_folder = os.listdir(TEST_DIR)\n",
    "\n",
    "# Count the number of files in the folder\n",
    "num_files_in_folder = len(files_in_folder)\n",
    "\n",
    "print(\"Number of files in the folder TEST_DIR:\", num_files_in_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape (103250, 12)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_DIR + 'all_data_info.csv') # r'\\train_info\\train_info.csv'\n",
    "print(\"df.shape\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>date</th>\n",
       "      <th>genre</th>\n",
       "      <th>pixelsx</th>\n",
       "      <th>pixelsy</th>\n",
       "      <th>size_bytes</th>\n",
       "      <th>source</th>\n",
       "      <th>style</th>\n",
       "      <th>title</th>\n",
       "      <th>artist_group</th>\n",
       "      <th>in_train</th>\n",
       "      <th>new_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Barnett Newman</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>abstract</td>\n",
       "      <td>15530.0</td>\n",
       "      <td>6911.0</td>\n",
       "      <td>9201912.0</td>\n",
       "      <td>wikiart</td>\n",
       "      <td>Color Field Painting</td>\n",
       "      <td>Uriel</td>\n",
       "      <td>train_only</td>\n",
       "      <td>True</td>\n",
       "      <td>102257.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Barnett Newman</td>\n",
       "      <td>1950.0</td>\n",
       "      <td>abstract</td>\n",
       "      <td>14559.0</td>\n",
       "      <td>6866.0</td>\n",
       "      <td>8867532.0</td>\n",
       "      <td>wikiart</td>\n",
       "      <td>Color Field Painting</td>\n",
       "      <td>Vir Heroicus Sublimis</td>\n",
       "      <td>train_only</td>\n",
       "      <td>True</td>\n",
       "      <td>75232.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kiri nichol</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9003.0</td>\n",
       "      <td>9004.0</td>\n",
       "      <td>1756681.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Neoplasticism</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test_only</td>\n",
       "      <td>False</td>\n",
       "      <td>32145.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kiri nichol</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9003.0</td>\n",
       "      <td>9004.0</td>\n",
       "      <td>1942046.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Neoplasticism</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test_only</td>\n",
       "      <td>False</td>\n",
       "      <td>20304.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kiri nichol</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9003.0</td>\n",
       "      <td>9004.0</td>\n",
       "      <td>1526212.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Neoplasticism</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test_only</td>\n",
       "      <td>False</td>\n",
       "      <td>836.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           artist    date     genre  pixelsx  pixelsy  size_bytes   source  \\\n",
       "0  Barnett Newman  1955.0  abstract  15530.0   6911.0   9201912.0  wikiart   \n",
       "1  Barnett Newman  1950.0  abstract  14559.0   6866.0   8867532.0  wikiart   \n",
       "2     kiri nichol  2013.0       NaN   9003.0   9004.0   1756681.0      NaN   \n",
       "3     kiri nichol  2013.0       NaN   9003.0   9004.0   1942046.0      NaN   \n",
       "4     kiri nichol  2013.0       NaN   9003.0   9004.0   1526212.0      NaN   \n",
       "\n",
       "                  style                  title artist_group  in_train  \\\n",
       "0  Color Field Painting                  Uriel   train_only      True   \n",
       "1  Color Field Painting  Vir Heroicus Sublimis   train_only      True   \n",
       "2         Neoplasticism                    NaN    test_only     False   \n",
       "3         Neoplasticism                    NaN    test_only     False   \n",
       "4         Neoplasticism                    NaN    test_only     False   \n",
       "\n",
       "  new_filename  \n",
       "0   102257.jpg  \n",
       "1    75232.jpg  \n",
       "2    32145.jpg  \n",
       "3    20304.jpg  \n",
       "4      836.jpg  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             artist    date              genre  pixelsx  \\\n",
      "185                  Giorgio Vasari    1534           portrait   3512.0   \n",
      "731                       Juan Gris    1918          cityscape   2844.0   \n",
      "994                       Juan Gris    1917         still life   3918.0   \n",
      "1084   John Roddam Spencer Stanhope    1860     genre painting   4249.0   \n",
      "1105                 Henri Rousseau    1908          cityscape   3403.0   \n",
      "9984   John Roddam Spencer Stanhope     NaN  symbolic painting   1309.0   \n",
      "19876              Georgia O'Keeffe  1915.0           abstract   1400.0   \n",
      "28748                    Franz Marc    1913    animal painting    951.0   \n",
      "46256           Esaias van de Velde    1622          landscape    942.0   \n",
      "98236                 George Inness     NaN  literary painting    376.0   \n",
      "\n",
      "       pixelsy  size_bytes     source                         style  \\\n",
      "185     4850.0  10360549.0    wikiart  Mannerism (Late Renaissance)   \n",
      "731     3882.0   5393504.0    wikiart                        Cubism   \n",
      "994     2424.0   5539240.0    wikiart                        Cubism   \n",
      "1084    2138.0   1897475.0    wikiart                   Romanticism   \n",
      "1105    2646.0   1491631.0    wikiart       Naïve Art (Primitivism)   \n",
      "9984    2235.0   1956142.0    wikiart                   Romanticism   \n",
      "19876    988.0    126931.0    wikiart                  Abstract Art   \n",
      "28748   1074.0    219940.0    wikiart                        Cubism   \n",
      "46256    790.0    110509.0  wikipedia                       Baroque   \n",
      "98236    500.0     79484.0  wikipedia                      Tonalism   \n",
      "\n",
      "                                                   title    artist_group  \\\n",
      "185                   Portrait of Alessandro de' Medici   train_and_test   \n",
      "731                                Landscape at Beaulieu  train_and_test   \n",
      "994                                     Glass and carafe  train_and_test   \n",
      "1084                               Robin of Modern Times      train_only   \n",
      "1105   View of the Bridge at Sevres and the Hills at ...  train_and_test   \n",
      "9984                                Thoughts of the Past      train_only   \n",
      "19876                                     Special No. 32  train_and_test   \n",
      "28748                                   Sleeping Animals  train_and_test   \n",
      "46256  A landscape with travellers crossing a bridge ...  train_and_test   \n",
      "98236                                   Cows by a Stream  train_and_test   \n",
      "\n",
      "       in_train new_filename  \n",
      "185       False    20153.jpg  \n",
      "731        True    41945.jpg  \n",
      "994        True   101947.jpg  \n",
      "1084       True    95347.jpg  \n",
      "1105      False    18649.jpg  \n",
      "9984       True    91033.jpg  \n",
      "19876      True    79499.jpg  \n",
      "28748     False   100532.jpg  \n",
      "46256      True    92899.jpg  \n",
      "98236      True     3917.jpg  \n",
      "Empty DataFrame\n",
      "Columns: [artist, date, genre, pixelsx, pixelsy, size_bytes, source, style, title, artist_group, in_train, new_filename]\n",
      "Index: []\n",
      "df.shape (103240, 12)\n"
     ]
    }
   ],
   "source": [
    "# quick fix for corrupted files\n",
    "list_of_corrupted = ['3917.jpg','18649.jpg','20153.jpg','41945.jpg',\n",
    "'79499.jpg','91033.jpg','92899.jpg','95347.jpg',\n",
    "'100532.jpg','101947.jpg']\n",
    "# display the corrupted rows of dataset for context\n",
    "corrupt_df = df[df[\"new_filename\"].isin(list_of_corrupted) == True]\n",
    "print(corrupt_df.head(len(list_of_corrupted)))\n",
    "\n",
    "# completely get rid of them\n",
    "df = df[df[\"new_filename\"].isin(list_of_corrupted) == False]\n",
    "\n",
    "# try to see if they are still there\n",
    "print(df[df[\"new_filename\"].isin(list_of_corrupted) == True])\n",
    "\n",
    "print(\"df.shape\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep the works of artists who have a representation above 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Count occurrences of each artist\n",
    "artist_counts = df['artist'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "500\n",
      "500\n",
      "500\n",
      "500\n",
      "500\n",
      "500\n",
      "500\n",
      "500\n",
      "500\n",
      "500\n",
      "500\n",
      "499\n",
      "499\n",
      "499\n",
      "499\n",
      "498\n",
      "498\n",
      "498\n",
      "497\n",
      "496\n",
      "496\n",
      "495\n",
      "495\n",
      "494\n",
      "493\n",
      "488\n",
      "485\n",
      "485\n",
      "464\n",
      "451\n",
      "449\n",
      "437\n",
      "427\n",
      "422\n",
      "412\n",
      "405\n",
      "388\n",
      "386\n",
      "379\n",
      "375\n",
      "369\n",
      "364\n",
      "364\n",
      "363\n",
      "360\n",
      "358\n",
      "351\n",
      "347\n",
      "344\n",
      "342\n",
      "335\n",
      "322\n",
      "321\n",
      "315\n",
      "305\n",
      "302\n",
      "285\n",
      "285\n",
      "284\n",
      "280\n",
      "275\n",
      "267\n",
      "265\n",
      "265\n",
      "262\n",
      "260\n",
      "258\n",
      "256\n",
      "254\n",
      "252\n",
      "250\n",
      "250\n",
      "249\n",
      "249\n",
      "245\n",
      "242\n",
      "240\n",
      "240\n",
      "240\n",
      "239\n",
      "237\n",
      "237\n",
      "237\n",
      "234\n",
      "234\n",
      "233\n",
      "231\n",
      "231\n",
      "229\n",
      "228\n",
      "226\n",
      "226\n",
      "224\n",
      "224\n",
      "222\n",
      "221\n",
      "218\n",
      "218\n",
      "217\n",
      "214\n",
      "213\n",
      "209\n",
      "207\n",
      "206\n",
      "206\n",
      "203\n",
      "203\n",
      "200\n",
      "200\n",
      "199\n",
      "197\n",
      "197\n",
      "196\n",
      "196\n",
      "195\n",
      "194\n",
      "194\n",
      "194\n",
      "193\n",
      "193\n",
      "187\n",
      "186\n",
      "186\n",
      "184\n",
      "184\n",
      "183\n",
      "182\n",
      "182\n",
      "181\n",
      "179\n",
      "178\n",
      "176\n",
      "175\n",
      "174\n",
      "174\n",
      "173\n",
      "172\n",
      "172\n",
      "172\n",
      "172\n",
      "168\n",
      "167\n",
      "167\n",
      "167\n",
      "167\n",
      "166\n",
      "164\n",
      "163\n",
      "163\n",
      "160\n",
      "160\n",
      "159\n",
      "158\n",
      "157\n",
      "156\n",
      "155\n",
      "155\n",
      "150\n",
      "149\n",
      "149\n",
      "148\n",
      "148\n",
      "147\n",
      "147\n",
      "146\n",
      "145\n",
      "145\n",
      "145\n",
      "144\n",
      "144\n",
      "143\n",
      "142\n",
      "142\n",
      "142\n",
      "139\n",
      "139\n",
      "139\n",
      "139\n",
      "137\n",
      "136\n",
      "135\n",
      "135\n",
      "134\n",
      "133\n",
      "133\n",
      "132\n",
      "132\n",
      "129\n",
      "128\n",
      "128\n",
      "128\n",
      "127\n",
      "127\n",
      "127\n",
      "126\n",
      "126\n",
      "126\n",
      "126\n",
      "126\n",
      "126\n",
      "126\n",
      "125\n",
      "124\n",
      "123\n",
      "122\n",
      "122\n",
      "122\n",
      "121\n",
      "121\n",
      "121\n",
      "121\n",
      "120\n",
      "120\n",
      "119\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "117\n",
      "117\n",
      "117\n",
      "115\n",
      "114\n",
      "114\n",
      "113\n",
      "113\n",
      "113\n",
      "113\n",
      "112\n",
      "112\n",
      "112\n",
      "111\n",
      "111\n",
      "110\n",
      "110\n",
      "110\n",
      "109\n",
      "108\n",
      "108\n",
      "107\n",
      "107\n",
      "107\n",
      "106\n",
      "106\n",
      "105\n",
      "104\n",
      "104\n",
      "104\n",
      "103\n",
      "102\n",
      "102\n",
      "102\n",
      "101\n",
      "101\n",
      "100\n",
      "100\n",
      "99\n",
      "99\n",
      "98\n",
      "97\n",
      "97\n",
      "97\n",
      "96\n",
      "96\n",
      "96\n",
      "96\n",
      "96\n",
      "96\n",
      "95\n",
      "95\n",
      "95\n",
      "94\n",
      "94\n",
      "94\n",
      "93\n",
      "93\n",
      "93\n",
      "93\n",
      "93\n",
      "93\n",
      "93\n",
      "92\n",
      "92\n",
      "92\n",
      "92\n",
      "92\n",
      "91\n",
      "91\n",
      "90\n",
      "90\n",
      "89\n",
      "89\n",
      "89\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "87\n",
      "87\n",
      "87\n",
      "87\n",
      "87\n",
      "87\n",
      "86\n",
      "86\n",
      "86\n",
      "86\n",
      "85\n",
      "85\n",
      "84\n",
      "84\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "82\n",
      "82\n",
      "82\n",
      "82\n",
      "82\n",
      "82\n",
      "81\n",
      "81\n",
      "80\n",
      "80\n",
      "80\n",
      "79\n",
      "79\n",
      "79\n",
      "79\n",
      "79\n",
      "78\n",
      "78\n",
      "78\n",
      "78\n",
      "78\n",
      "78\n",
      "78\n",
      "78\n",
      "77\n",
      "77\n",
      "77\n",
      "77\n",
      "77\n",
      "77\n",
      "77\n",
      "76\n",
      "76\n",
      "75\n",
      "75\n",
      "75\n",
      "74\n",
      "74\n",
      "74\n",
      "74\n",
      "74\n",
      "73\n",
      "73\n",
      "73\n",
      "73\n",
      "72\n",
      "72\n",
      "72\n",
      "72\n",
      "71\n",
      "71\n",
      "71\n",
      "71\n",
      "71\n",
      "71\n",
      "70\n",
      "70\n",
      "70\n",
      "70\n",
      "70\n",
      "69\n",
      "69\n",
      "69\n",
      "69\n",
      "68\n",
      "68\n",
      "68\n",
      "68\n",
      "68\n",
      "68\n",
      "68\n",
      "68\n",
      "68\n",
      "68\n",
      "68\n",
      "67\n",
      "67\n",
      "67\n",
      "67\n",
      "67\n",
      "67\n",
      "67\n",
      "66\n",
      "66\n",
      "66\n",
      "66\n",
      "66\n",
      "66\n",
      "66\n",
      "65\n",
      "65\n",
      "65\n",
      "65\n",
      "65\n",
      "64\n",
      "64\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "63\n",
      "62\n",
      "62\n",
      "62\n",
      "62\n",
      "62\n",
      "62\n",
      "62\n",
      "62\n",
      "62\n",
      "62\n",
      "62\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "59\n",
      "59\n",
      "59\n",
      "59\n",
      "59\n",
      "59\n",
      "59\n",
      "58\n",
      "58\n",
      "58\n",
      "58\n",
      "58\n",
      "58\n",
      "58\n",
      "58\n",
      "57\n",
      "57\n",
      "57\n",
      "57\n",
      "57\n",
      "57\n",
      "56\n",
      "56\n",
      "56\n",
      "56\n",
      "56\n",
      "56\n",
      "56\n",
      "56\n",
      "56\n",
      "56\n",
      "56\n",
      "56\n",
      "55\n",
      "55\n",
      "55\n",
      "55\n",
      "55\n",
      "55\n",
      "55\n",
      "54\n",
      "54\n",
      "54\n",
      "54\n",
      "54\n",
      "54\n",
      "54\n",
      "54\n",
      "53\n",
      "53\n",
      "53\n",
      "53\n",
      "53\n",
      "52\n",
      "52\n",
      "52\n",
      "52\n",
      "52\n",
      "52\n",
      "52\n",
      "52\n",
      "51\n",
      "51\n",
      "51\n",
      "51\n",
      "51\n",
      "51\n",
      "51\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "49\n",
      "49\n",
      "49\n",
      "49\n",
      "48\n",
      "48\n",
      "48\n",
      "48\n",
      "48\n",
      "48\n",
      "48\n",
      "48\n",
      "48\n",
      "48\n",
      "48\n",
      "48\n",
      "48\n",
      "47\n",
      "47\n",
      "47\n",
      "47\n",
      "47\n",
      "47\n",
      "47\n",
      "47\n",
      "46\n",
      "46\n",
      "46\n",
      "46\n",
      "46\n",
      "46\n",
      "46\n",
      "46\n",
      "46\n",
      "46\n",
      "45\n",
      "45\n",
      "45\n",
      "45\n",
      "45\n",
      "45\n",
      "45\n",
      "45\n",
      "45\n",
      "45\n",
      "45\n",
      "45\n",
      "45\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "43\n",
      "43\n",
      "43\n",
      "43\n",
      "43\n",
      "43\n",
      "43\n",
      "43\n",
      "43\n",
      "43\n",
      "43\n",
      "43\n",
      "42\n",
      "42\n",
      "42\n",
      "42\n",
      "42\n",
      "42\n",
      "42\n",
      "42\n",
      "42\n",
      "42\n",
      "42\n",
      "42\n",
      "42\n",
      "42\n",
      "42\n",
      "42\n",
      "42\n",
      "42\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "39\n",
      "39\n",
      "39\n",
      "39\n",
      "39\n",
      "39\n",
      "39\n",
      "39\n",
      "39\n",
      "39\n",
      "39\n",
      "39\n",
      "38\n",
      "38\n",
      "38\n",
      "38\n",
      "38\n",
      "38\n",
      "38\n",
      "38\n",
      "38\n",
      "38\n",
      "38\n",
      "38\n",
      "38\n",
      "38\n",
      "38\n",
      "38\n",
      "38\n",
      "37\n",
      "37\n",
      "37\n",
      "37\n",
      "37\n",
      "37\n",
      "37\n",
      "37\n",
      "37\n",
      "37\n",
      "37\n",
      "37\n",
      "37\n",
      "37\n",
      "37\n",
      "37\n",
      "37\n",
      "37\n",
      "37\n",
      "37\n",
      "36\n",
      "36\n",
      "36\n",
      "36\n",
      "36\n",
      "36\n",
      "36\n",
      "36\n",
      "36\n",
      "36\n",
      "36\n",
      "36\n",
      "36\n",
      "36\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "34\n",
      "34\n",
      "34\n",
      "34\n",
      "34\n",
      "34\n",
      "34\n",
      "34\n",
      "34\n",
      "34\n",
      "34\n",
      "34\n",
      "34\n",
      "34\n",
      "34\n",
      "34\n",
      "34\n",
      "34\n",
      "34\n",
      "34\n",
      "34\n",
      "34\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "29\n",
      "29\n",
      "29\n",
      "29\n",
      "29\n",
      "29\n",
      "29\n",
      "29\n",
      "29\n",
      "29\n",
      "29\n",
      "29\n",
      "29\n",
      "29\n",
      "29\n",
      "29\n",
      "29\n",
      "29\n",
      "29\n",
      "29\n",
      "29\n",
      "29\n",
      "29\n",
      "29\n",
      "29\n",
      "29\n",
      "29\n",
      "29\n",
      "29\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for artist in artist_counts:\n",
    "    print(artist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above 499 artistic works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists_above_499 = artist_counts[artist_counts >= 499].index\n",
    "filtered_df = df[df['artist'].isin(artists_above_499)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above 199 artistic works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists_above_99 = artist_counts[artist_counts >= 99].index\n",
    "filtered_df = df[df['artist'].isin(artists_above_99)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep files in directories of the filtered artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "\n",
    "# Define the input folder\n",
    "#input_folder = r\"C:\\Users\\Mercè\\Documents\\UAB\\XN\\Projecte\\input\"\n",
    "\n",
    "input_folder = \"../../train1_petit_total\"\n",
    "\n",
    "# Define the output folder where you want to move the files\n",
    "#output_folder = r\"C:\\Users\\Mercè\\Documents\\UAB\\XN\\Projecte\\input_new\"\n",
    "\n",
    "output_folder = \"../../train1_petit_total_output\"\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate over the rows in the filtered DataFrame\n",
    "for index, row in filtered_df.iterrows():\n",
    "    # Extract the new_filename from the DataFrame\n",
    "    new_filename = row['new_filename']\n",
    "    \n",
    "    # Determine the subdirectory of the file (train_1 or test)\n",
    "    source_subdirectory = \"train_1\" \n",
    "    # source_subdirectory = \"test\"\n",
    "\n",
    "    # Search for the file in the input folder and its subdirectories\n",
    "    for root, dirs, files in os.walk(os.path.join(input_folder, source_subdirectory)):\n",
    "        if new_filename in files:\n",
    "            # Get the full path of the file\n",
    "            file_path = os.path.join(root, new_filename)\n",
    "            \n",
    "            # Determine the destination subdirectory\n",
    "            destination_subdirectory = os.path.join(output_folder, source_subdirectory)\n",
    "            \n",
    "            # Create the destination subdirectory if it doesn't exist\n",
    "            os.makedirs(destination_subdirectory, exist_ok=True)\n",
    "            \n",
    "            # Define the destination path where you want to move the file\n",
    "            destination_path = os.path.join(destination_subdirectory, new_filename)\n",
    "            \n",
    "            # Move the file to the output folder\n",
    "            shutil.move(file_path, destination_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input folder\n",
    "#input_folder = r\"C:\\Users\\Mercè\\Documents\\UAB\\XN\\Projecte\\input\"\n",
    "\n",
    "input_folder = \"../../train1_petit_total\"\n",
    "\n",
    "# Define the output folder where you want to move the files\n",
    "#output_folder = r\"C:\\Users\\Mercè\\Documents\\UAB\\XN\\Projecte\\input_new\"\n",
    "\n",
    "output_folder = \"../../train1_petit_total_output\"\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate over the rows in the filtered DataFrame\n",
    "for index, row in filtered_df.iterrows():\n",
    "    # Extract the new_filename from the DataFrame\n",
    "    new_filename = row['new_filename']\n",
    "    \n",
    "    # Determine the subdirectory of the file (train_1 or test)\n",
    "    # source_subdirectory = \"train_1\" \n",
    "    source_subdirectory = \"test\"\n",
    "\n",
    "    # Search for the file in the input folder and its subdirectories\n",
    "    for root, dirs, files in os.walk(os.path.join(input_folder, source_subdirectory)):\n",
    "        if new_filename in files:\n",
    "            # Get the full path of the file\n",
    "            file_path = os.path.join(root, new_filename)\n",
    "            \n",
    "            # Determine the destination subdirectory\n",
    "            destination_subdirectory = os.path.join(output_folder, source_subdirectory)\n",
    "            \n",
    "            # Create the destination subdirectory if it doesn't exist\n",
    "            os.makedirs(destination_subdirectory, exist_ok=True)\n",
    "            \n",
    "            # Define the destination path where you want to move the file\n",
    "            destination_path = os.path.join(destination_subdirectory, new_filename)\n",
    "            \n",
    "            # Move the file to the output folder\n",
    "            shutil.move(file_path, destination_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in the folder TRAIN_1_DIR: 12\n",
      "Number of files in the folder TEST_DIR: 0\n"
     ]
    }
   ],
   "source": [
    "# Get the list of files in the folder\n",
    "files_in_folder = os.listdir(TRAIN_1_DIR)\n",
    "\n",
    "# Count the number of files in the folder\n",
    "num_files_in_folder = len(files_in_folder)\n",
    "\n",
    "print(\"Number of files in the folder TRAIN_1_DIR:\", num_files_in_folder)\n",
    "\n",
    "files_in_folder = os.listdir(TEST_DIR)\n",
    "\n",
    "# Count the number of files in the folder\n",
    "num_files_in_folder = len(files_in_folder)\n",
    "\n",
    "print(\"Number of files in the folder TEST_DIR:\", num_files_in_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_df.shape (23814, 2)\n",
      "train_df.shape (103240, 2)\n",
      "number of artists 110\n",
      "\n",
      "list of artists...\n",
      " ['Ivan Aivazovsky', 'Jan Matejko', 'Gustave Dore', 'Titian', 'Lucas Cranach the Elder', 'Utagawa Kuniyoshi', 'Odilon Redon', 'William Turner', 'Henri de Toulouse-Lautrec', 'Konstantin Somov', 'Thomas Eakins', 'Gustave Caillebotte', 'Ferdinand Hodler', 'William-Adolphe Bouguereau', 'Rembrandt', 'Ernst Ludwig Kirchner', 'Joshua Reynolds', 'William Merritt Chase', 'Edgar Degas', 'Claude Monet', 'Theodor Severin Kittelsen', 'Valentin Serov', 'Felix Vallotton ', 'Pierre-Auguste Renoir', 'Albrecht Durer', 'Francisco Goya', 'Theophile Steinlen', 'Katsushika Hokusai', 'Tintoretto', 'Orest Kiprensky', 'Alfred Sisley', 'Ivan Bilibin', 'Berthe Morisot', 'Honore Daumier', 'Ivan Shishkin', 'Giovanni Battista Piranesi', 'Camille Corot', 'Childe Hassam', 'Raphael Kirchner', 'James Tissot', 'Giovanni Boldini', 'Vasily Vereshchagin', 'Niko Pirosmani', 'Eugene Boudin', 'Paul Cezanne', 'John Singer Sargent', 'Peter Paul Rubens', 'Vasily Surikov', 'Vincent van Gogh', 'Georges Braque', 'Eugene Delacroix', 'Isaac Levitan', 'Louis Comfort Tiffany', 'Jean Auguste Dominique Ingres', 'Zdislav Beksinski', 'Gustave Loiseau', 'David Burliuk', 'Camille Pissarro', 'Gustave Courbet', 'Fernand Leger', 'Amedeo Modigliani', 'Henri Fantin-Latour', 'Konstantin Makovsky', 'Edouard Manet', 'Robert Henri', 'Konstantin Yuon', 'Mary Cassatt', 'Sam Francis', 'Boris Kustodiev', 'Kuzma Petrov-Vodkin', 'John Henry Twachtman', 'Egon Schiele', 'Nicholas Roerich', 'Ilya Repin', 'Aubrey Beardsley', 'James McNeill Whistler', 'Paul Klee', 'Edouard Cortes', 'Martiros Saryan', 'Pyotr Konchalovsky', 'Pierre Bonnard', 'M.C. Escher', 'Maurice Prendergast', 'Salvador Dali', 'Pablo Picasso', 'Henri Matisse', 'Ilya Mashkov', 'Vasily Polenov', 'Rene Magritte', 'Norman Rockwell', ' Wassily Kandinsky', 'Max Ernst', 'Henri Martin', \"Georgia O'Keeffe\", 'Erte', 'Marc Chagall', 'Paul Gauguin', 'Albert Bierstadt', 'Aleksey Savrasov', 'Zinaida Serebriakova', 'Eyvind Earle', 'Gene Davis', 'Konstantin Korovin', 'Lucian Freud', 'Charles Turner', 'Samuel Peploe', 'Charles M. Russell', 'George Stefanescu-Ramnic ', 'George Romney', 'Thomas Lawrence']\n"
     ]
    }
   ],
   "source": [
    "train_df = df[df[\"in_train\"] == True]\n",
    "test_df = df[df['in_train'] == False]\n",
    "train_df = df[['artist', 'new_filename']]\n",
    "test_df = test_df[['artist', 'new_filename']]\n",
    "\n",
    "print(\"test_df.shape\", test_df.shape)\n",
    "print(\"train_df.shape\", train_df.shape)\n",
    "\n",
    "artists = {} # holds artist hash & the count\n",
    "for a in train_df['artist']:\n",
    "    if (a not in artists):\n",
    "        artists[a] = 1\n",
    "    else:\n",
    "        artists[a] += 1\n",
    "\n",
    "training_set_artists = []\n",
    "for a,count in artists.items():\n",
    "    if(int(count) >= 200): # it takes no more than 200 artists\n",
    "        training_set_artists.append(a)\n",
    "\n",
    "print(\"number of artists\",len(training_set_artists))\n",
    "\n",
    "print(\"\\nlist of artists...\\n\", training_set_artists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>new_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Ivan Aivazovsky</td>\n",
       "      <td>99442.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Jan Matejko</td>\n",
       "      <td>75956.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Gustave Dore</td>\n",
       "      <td>7486.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Gustave Dore</td>\n",
       "      <td>35766.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Gustave Dore</td>\n",
       "      <td>31977.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             artist new_filename\n",
       "14  Ivan Aivazovsky    99442.jpg\n",
       "25      Jan Matejko    75956.jpg\n",
       "28     Gustave Dore     7486.jpg\n",
       "29     Gustave Dore    35766.jpg\n",
       "34     Gustave Dore    31977.jpg"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_df = train_df[train_df[\"artist\"].isin(training_set_artists)]\n",
    "\n",
    "t_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>new_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Ivan Aivazovsky</td>\n",
       "      <td>99442.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Utagawa Kuniyoshi</td>\n",
       "      <td>99733.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Utagawa Kuniyoshi</td>\n",
       "      <td>93715.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Ferdinand Hodler</td>\n",
       "      <td>96174.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Joshua Reynolds</td>\n",
       "      <td>91879.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                artist new_filename\n",
       "14     Ivan Aivazovsky    99442.jpg\n",
       "49   Utagawa Kuniyoshi    99733.jpg\n",
       "51   Utagawa Kuniyoshi    93715.jpg\n",
       "97    Ferdinand Hodler    96174.jpg\n",
       "127    Joshua Reynolds    91879.jpg"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1_df = t_df[t_df['new_filename'].str.startswith('1')]\n",
    "\n",
    "t2_df = t_df[t_df['new_filename'].str.startswith('2')]\n",
    "\n",
    "t3_df = t_df[t_df['new_filename'].str.startswith('3')]\n",
    "\n",
    "t4_df = t_df[t_df['new_filename'].str.startswith('4')]\n",
    "\n",
    "t5_df = t_df[t_df['new_filename'].str.startswith('5')]\n",
    "\n",
    "t6_df = t_df[t_df['new_filename'].str.startswith('6')]\n",
    "\n",
    "t7_df = t_df[t_df['new_filename'].str.startswith('7')]\n",
    "\n",
    "t8_df = t_df[t_df['new_filename'].str.startswith('8')]\n",
    "\n",
    "t9_df = t_df[t_df['new_filename'].str.startswith('9')]\n",
    "\n",
    "all_train_dfs = [t1_df, t2_df, t3_df,\n",
    "                t4_df, t5_df, t6_df,\n",
    "                t7_df, t8_df, t9_df]\n",
    "\n",
    "t9_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# specify the model that classifies 38 artists 🎨 🖌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_set_artists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(training_set_artists) # one class per artist\n",
    "# weights_notop_path = r\"C:\\Users\\Mercè\\Documents\\UAB\\XN\\Projecte\\resnet50\\resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "model_adam = Sequential()\n",
    "\n",
    "model_adam.add(ResNet50(\n",
    "  include_top=False,\n",
    "  weights='imagenet',\n",
    "  pooling='avg'\n",
    "))\n",
    "model_adam.add(Dense(\n",
    "  num_classes,\n",
    "  activation='softmax'\n",
    "))\n",
    "\n",
    "model_adam.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_adam.compile(\n",
    "  optimizer='adam', # lots of people reccommend Adam optimizer\n",
    "  loss='categorical_crossentropy', # aka \"log loss\" -- the cost function to minimize \n",
    "  # so 'optimizer' algorithm will minimize 'loss' function\n",
    "  metrics=['accuracy'] # ask it to report % of correct predictions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "para un problema de clasificación binaria, a menudo se utiliza la 'entropía cruzada binaria', mientras que la 'entropía cruzada categórica' se utiliza para la clasificación de clases múltiples.\n",
    "https://www.sourcetrail.com/es/pit%C3%B3n/keras/modelo-compilar-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the image data generator for each training directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model globals\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 96\n",
    "TEST_BATCH_SIZE = 17 # because test has 23817 images and factors of 23817 are 3*17*467\n",
    "                     # it is important that this number evenly divides the total num images \n",
    "VAL_SPLIT = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defined setup_generators()\n"
     ]
    }
   ],
   "source": [
    "def setup_generators(\n",
    "    val_split, train_dataframe, train_dir,\n",
    "    img_size, batch_size, my_seed, list_of_classes,\n",
    "    test_dataframe, test_dir, test_batch_size\n",
    "):\n",
    "    print(\"-\"*20)\n",
    "    if not preprocess_input:\n",
    "          raise Exception(\"please do import call 'from tensorflow.python.keras.applications.resnet50 import preprocess_input'\")\n",
    "\n",
    "    # setup resnet50 preprocessing \n",
    "    data_gen = ImageDataGenerator(\n",
    "        preprocessing_function=preprocess_input,\n",
    "        validation_split=val_split)\n",
    "\n",
    "    print(len(train_dataframe), \"images in\", train_dir, \"and validation_split =\", val_split)\n",
    "    print(\"\\ntraining set ImageDataGenerator\")\n",
    "    train_gen = data_gen.flow_from_dataframe(\n",
    "        dataframe=train_dataframe.reset_index(), # call reset_index() so keras can start with index 0\n",
    "        directory=train_dir,\n",
    "        x_col='new_filename',\n",
    "        y_col='artist',\n",
    "        has_ext=True,\n",
    "        target_size=(img_size, img_size),\n",
    "        subset=\"training\",\n",
    "        batch_size=batch_size,\n",
    "        seed=my_seed,\n",
    "        shuffle=True,\n",
    "        class_mode='categorical',\n",
    "        classes=list_of_classes\n",
    "    )\n",
    "\n",
    "    print(\"\\nvalidation set ImageDataGenerator\")\n",
    "    valid_gen = data_gen.flow_from_dataframe(\n",
    "        dataframe=train_dataframe.reset_index(), # call reset_index() so keras can start with index 0\n",
    "        directory=train_dir,\n",
    "        x_col='new_filename',\n",
    "        y_col='artist',\n",
    "        has_ext=True,\n",
    "        subset=\"validation\",\n",
    "        batch_size=batch_size,\n",
    "        seed=my_seed,\n",
    "        shuffle=True,\n",
    "        target_size=(img_size,img_size),\n",
    "        class_mode='categorical',\n",
    "        classes=list_of_classes\n",
    "    )\n",
    "\n",
    "    test_data_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "    print(\"\\ntest set ImageDataGenerator\")\n",
    "    test_gen = test_data_gen.flow_from_dataframe(\n",
    "        dataframe=test_dataframe.reset_index(), # call reset_index() so keras can start with index 0\n",
    "        directory=test_dir,\n",
    "        x_col='new_filename',\n",
    "        y_col=None,\n",
    "        has_ext=True,\n",
    "        batch_size=test_batch_size,\n",
    "        seed=my_seed,\n",
    "        shuffle=False, # dont shuffle test directory\n",
    "        class_mode=None,\n",
    "        target_size=(img_size,img_size)\n",
    "    )\n",
    "\n",
    "    return (train_gen, valid_gen, test_gen)\n",
    "\n",
    "print(\"defined setup_generators()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1017"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete some unused dataframes to free some RAM for training\n",
    "del df\n",
    "del t_df\n",
    "del t1_df\n",
    "del t2_df\n",
    "del t3_df\n",
    "del t4_df\n",
    "del t5_df\n",
    "del t6_df\n",
    "del t7_df\n",
    "del t8_df\n",
    "del t9_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "5230 images in ../../train_10_artists and validation_split = 0.25\n",
      "\n",
      "training set ImageDataGenerator\n",
      "Found 1 validated image filenames belonging to 110 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aluce\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:920: UserWarning: Found 5229 invalid image filename(s) in x_col=\"new_filename\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation set ImageDataGenerator\n",
      "Found 0 validated image filenames belonging to 110 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aluce\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:920: UserWarning: Found 5229 invalid image filename(s) in x_col=\"new_filename\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test set ImageDataGenerator\n",
      "Found 0 validated image filenames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aluce\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:920: UserWarning: Found 23814 invalid image filename(s) in x_col=\"new_filename\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_gens = [None]*len(TRAIN_DIRS)\n",
    "valid_gens = [None]*len(TRAIN_DIRS)\n",
    "test_gen  = None # only 1 test_gen\n",
    "i = 0\n",
    "for i in range(0, len(TRAIN_DIRS)):\n",
    "    train_gens[i], valid_gens[i], test_gen = setup_generators(\n",
    "        train_dataframe=all_train_dfs[i], train_dir=TRAIN_DIRS[i],\n",
    "        val_split=VAL_SPLIT, img_size=IMAGE_SIZE, batch_size=BATCH_SIZE, my_seed=MY_SEED, \n",
    "        list_of_classes=training_set_artists, test_dataframe=test_df, \n",
    "        test_dir=TEST_DIR, test_batch_size=TEST_BATCH_SIZE\n",
    "    )\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING TIME!  🎉 🎊 🎁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 5 * len(train_gens) # should be a multiple of 9 because need evenly train each train_dir\n",
    "DIR_EPOCHS = 1 # fit each train_dir at least this many times before overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP_SIZE_TRAIN 0\n",
      "STEP_SIZE_VALID 0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - accuracy: 0.0000e+00 - loss: 4.0627"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aluce\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must provide at least one structure",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTEP_SIZE_TRAIN\u001b[39m\u001b[38;5;124m\"\u001b[39m,STEP_SIZE_TRAIN)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTEP_SIZE_VALID\u001b[39m\u001b[38;5;124m\"\u001b[39m,STEP_SIZE_VALID)\n\u001b[0;32m     12\u001b[0m histories_adam\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m---> 13\u001b[0m     \u001b[43mmodel_adam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_gens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSTEP_SIZE_TRAIN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_gens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSTEP_SIZE_VALID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDIR_EPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     19\u001b[0m e\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\src\\tree\\optree_impl.py:76\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structures)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`func` must be callable. Received: func=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m structures:\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust provide at least one structure\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m     78\u001b[0m     assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: Must provide at least one structure"
     ]
    }
   ],
   "source": [
    "histories_adam = []\n",
    "\n",
    "e = 0\n",
    "while e < MAX_EPOCHS:\n",
    "    for i in range(len(train_gens)):\n",
    "        # Verifica que los generadores no sean None\n",
    "        if train_gens[i] is None or valid_gens[i] is None:\n",
    "            print(f\"Generador {i} es None\")\n",
    "            continue\n",
    "        \n",
    "        # Verifica que los generadores no estén vacíos\n",
    "        if train_gens[i].n == 0 or valid_gens[i].n == 0:\n",
    "            print(f\"Generador {i} está vacío\")\n",
    "            continue\n",
    "        \n",
    "        # Calcula los pasos por época para entrenamiento y validación\n",
    "        STEP_SIZE_TRAIN = train_gens[i].n // train_gens[i].batch_size\n",
    "        STEP_SIZE_VALID = valid_gens[i].n // valid_gens[i].batch_size\n",
    "\n",
    "        print(f\"Generador {i}:\")\n",
    "        print(\"STEP_SIZE_TRAIN:\", STEP_SIZE_TRAIN)\n",
    "        print(\"STEP_SIZE_VALID:\", STEP_SIZE_VALID)\n",
    "\n",
    "        # Verifica que STEP_SIZE_TRAIN y STEP_SIZE_VALID sean mayores que 0\n",
    "        if STEP_SIZE_TRAIN <= 0 or STEP_SIZE_VALID <= 0:\n",
    "            print(f\"Paso de entrenamiento o validación no válido para generador {i}\")\n",
    "            continue\n",
    "        \n",
    "        histories_adam.append(\n",
    "            model_adam.fit(\n",
    "                train_gens[i],\n",
    "                steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                validation_data=valid_gens[i],\n",
    "                validation_steps=STEP_SIZE_VALID,\n",
    "                epochs=DIR_EPOCHS\n",
    "            )\n",
    "        )\n",
    "        e += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model 🧐 🤔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for history in histories_adam:\n",
    "    print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_adam = []\n",
    "val_accuracies_adam = []\n",
    "losses_adam = []\n",
    "val_losses_adam = []\n",
    "for hist in histories_adam:\n",
    "    if hist:\n",
    "        accuracies_adam += hist.history['accuracy']\n",
    "        val_accuracies_adam += hist.history['val_accuracy']\n",
    "        losses_adam += hist.history['loss']\n",
    "        val_losses_adam += hist.history['val_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots\n",
    "### Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(accuracies_adam, label = \"Adam Train\")\n",
    "plt.plot(val_accuracies_adam, label = \"Adam Test\")\n",
    "\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(r'/Users/jesus/Desktop/Projecte/XNAPproject-grup07/ouput/model_acc_631_images.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(losses_adam, label = \"Adam Train\")\n",
    "plt.plot(val_losses_adam, label = \"Adam Test\")\n",
    "\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(r'/Users/jesus/Desktop/Projecte/XNAPproject-grup07/ouput/model_loss_631_images.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# timestr = time.strftime(\"%Y%m%d-%H%M%S\") # e.g: 20181109-180140\n",
    "# model_adam.save('painters_adam_e45_'+timestr+'.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the output 🔮 🎩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_STEPS = len(test_gen) # 100 # default would have been len(test_gen)\n",
    "PRED_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to reset the test_gen before calling predict_generator\n",
    "# This is important because forgetting to reset the test_generator results in outputs with a weird order.\n",
    "test_gen.reset()\n",
    "pred_adam = model_adam.predict(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pred_adam),\"\\n\",pred_adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_indices_adam = np.argmax(pred_adam,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_results(predicted_class_indices, train_gens):\n",
    "    print(len(predicted_class_indices),\"\\n\",predicted_class_indices)\n",
    "    print(\"it has values ranging from \",min(predicted_class_indices),\"...to...\",max(predicted_class_indices))\n",
    "    labels = (train_gens[0].class_indices)\n",
    "    labels = dict((v,k) for k,v in labels.items())\n",
    "    predictions = [labels[k] for k in predicted_class_indices]\n",
    "    print(\"*\"*20+\"\\nclass_indices\\n\"+\"*\"*20+\"\\n\",train_gens[0].class_indices,\"\\n\")\n",
    "    print(\"*\"*20+\"\\nlabels\\n\"+\"*\"*20+\"\\n\",labels,\"\\n\")\n",
    "    print(\"*\"*20+\"\\npredictions has\", len(predictions),\"values that look like\",\"'\"+str(predictions[0])+\"' which is the first prediction and corresponds to this index of the classes:\",train_gens[0].class_indices[predictions[0]])\n",
    "    # Save the results to a CSV file.\n",
    "    filenames=test_gen.filenames[:len(predictions)] # because \"ValueError: arrays must all be same length\"\n",
    "\n",
    "    real_artists = []\n",
    "    for f in filenames:\n",
    "        real = test_df[test_df['new_filename'] == f].artist.to_numpy()[0]\n",
    "        real_artists.append(real)\n",
    "\n",
    "    results=pd.DataFrame({\"Filename\":filenames,\n",
    "                        \"Predictions\":predictions,\n",
    "                        \"Real Values\":real_artists})\n",
    "    results.to_csv(\"results.csv\",index=False)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_adam = retrieve_results(predicted_class_indices_adam, train_gens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_adam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training_set_artists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_set_artists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_new_images(results, training_set_artists):  \n",
    "    count = 0\n",
    "    match = 0\n",
    "    unexpected_count = 0\n",
    "    unexpected_match = 0\n",
    "    match_both_expected_unexpected = 0\n",
    "\n",
    "    for p, r in zip(results['Predictions'], results['Real Values']):\n",
    "        if r in training_set_artists:\n",
    "            count += 1\n",
    "            if p == r:\n",
    "                match += 1\n",
    "        else:\n",
    "            unexpected_count += 1\n",
    "            if p == r:\n",
    "                unexpected_match += 1\n",
    "\n",
    "    print(\"test accuracy on new images for TRAINED artists\")\n",
    "    acc = match/count\n",
    "    print(match,\"/\",count,\"=\",\"{:.4f}\".format(acc))\n",
    "\n",
    "    print(\"test accuracy on new images for UNEXPECTED artists\")\n",
    "    print(\"unexpected_match\", unexpected_match)\n",
    "    print(\"unexpected_count\", unexpected_count)\n",
    "    \n",
    "    if unexpected_count != 0:\n",
    "        u_acc = unexpected_match/unexpected_count\n",
    "        print(unexpected_match,\"/\",unexpected_count,\"=\",\"{:.4f}\".format(u_acc))\n",
    "\n",
    "    print(\"test accuracy on new images\")\n",
    "    total_match = match+unexpected_match\n",
    "    total_count = count+unexpected_count\n",
    "    total_acc = (total_match)/(total_count)\n",
    "    print(total_match,\"/\",total_count,\"=\",\"{:.4f}\".format(total_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_new_images(results_adam, training_set_artists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_set_artists))\n",
    "print(training_set_artists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_set_artists))\n",
    "print(training_set_artists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas\n",
    "- Having a the prediction dataset: Filename, Predictions, Real Values. We can construct a composition of the predictions.\n",
    "- We can now predict artist and another caracteristic (like stryle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_position(current_index, total_images_per_row, image_width, image_height, margin):\n",
    "    row = current_index // total_images_per_row\n",
    "    col = current_index % total_images_per_row\n",
    "    x = col * (image_width + margin)\n",
    "    y = row * (image_height + margin)\n",
    "    return (x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(TEST_DIR + \"\\\\\" + row['Filename']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_position(current_index, total_images_per_row, image_width, image_height, margin):\n",
    "    row = current_index // total_images_per_row\n",
    "    col = current_index % total_images_per_row\n",
    "    x = col * (image_width + margin)\n",
    "    y = row * (image_height + margin)\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# Group by predicted artist\n",
    "grouped_data = results_adam.groupby('Predictions') # tmb podria ser Real Values\n",
    "\n",
    "# Parameters for composition layout\n",
    "total_images_per_row = 5\n",
    "image_width = 200\n",
    "image_height = 200\n",
    "margin = 10\n",
    "\n",
    "# Create compositions\n",
    "# Create compositions\n",
    "for predicted_artist, group in grouped_data:\n",
    "    # Create a new blank image to compose the group of images\n",
    "    composition_width = total_images_per_row * (image_width + margin) - margin\n",
    "    num_images = len(group)\n",
    "    num_rows = (num_images + total_images_per_row - 1) // total_images_per_row\n",
    "    title_height = 30  # Height reserved for the title\n",
    "    composition_height = num_rows * (image_height + margin) - margin + title_height\n",
    "    composition = Image.new('RGB', (composition_width, composition_height), color='white')\n",
    "\n",
    "    # Add the title\n",
    "    draw = ImageDraw.Draw(composition)\n",
    "    font = ImageFont.truetype(\"arial.ttf\", 20)\n",
    "    title = \"Predicted Artist: {}\".format(predicted_artist)\n",
    "    title_width = draw.textlength(title, font=font)\n",
    "    draw.text(((composition_width - title_width) // 2, 0), title, fill=\"black\", font=font)\n",
    "\n",
    "    # Loop through images in the group\n",
    "    for i, (_, row) in enumerate(group.iterrows()):\n",
    "        # Open the image file\n",
    "        image = Image.open(TEST_DIR + \"\\\\\" + row['Filename'])\n",
    "        \n",
    "        # Resize image if needed\n",
    "        image = image.resize((image_width, image_height))\n",
    "        \n",
    "        # Calculate position to paste this image in the composition\n",
    "        position = calculate_position(i, total_images_per_row, image_width, image_height, margin)\n",
    "        position = (position[0], position[1] + title_height)  # Adjust position for title\n",
    "        \n",
    "        # Paste the image onto the composition\n",
    "        composition.paste(image, position)\n",
    "    \n",
    "    # Display or save the composition\n",
    "    composition.save(r'C:\\Users\\Mercè\\Documents\\UAB\\XN\\Projecte\\XNAPproject-grup07\\compostions\\predictions_631_images\\composition_{}.png'.format(predicted_artist))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
